# -*- coding: utf-8 -*-
"""Ecom_case_study_CRM_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SdRuwM11bbh5a6ZuKwXEjnfMecBZE9-O

Involve a preliminary focus on refining the dataset through essential data preprocessing steps, encompassing outlier management, addressing missing values, and handling potential duplicates.
"""

import pandas as pd
df = pd.read_csv("/content/sample_data/data/Ecom_CRM_analysis.csv", encoding='ISO-8859-1')

df.head()

df.info()

"""there are no null values, but Nan values in Descrition and CustermerID, to deal with them will replace them.

"""

df['Description'] = df['Description'].fillna("unknown")
df['CustomerID'] = df['CustomerID'].fillna(-1)
df.info()

"""**Feature Engineering**

"""

df['Cost'] = df['Quantity']*df['UnitPrice'].astype(float)
df.info()

"""**Data type change**"""

df['InvoiceNo'] = df['InvoiceNo'].astype(int, errors='ignore')

df.info()

"""**Handling Outliers**"""

Q1 = df['Cost'].quantile(0.25)
Q3 = df['Cost'].quantile(0.75)
IQR = Q3 - Q1

# Define the outlier range
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df['Cost'] < lower_bound) | (df['Cost'] > upper_bound)]
print(outliers)

# Calculate Q1, Q3, and IQR
Q1 = df['Cost'].quantile(0.25)
Q3 = df['Cost'].quantile(0.75)
IQR = Q3 - Q1

# Define the outlier range
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df['Cost'] < lower_bound) | (df['Cost'] > upper_bound)]
print("Number of outliers before imputation:", len(outliers))

# Calculate the median of the 'Cost' column
median_value = df['Cost'].median()

# Replace the outliers with the median value
df.loc[(df['Cost'] < lower_bound) | (df['Cost'] > upper_bound), 'Cost'] = median_value

# Check for remaining outliers
outliers_after_imputation = df[(df['Cost'] < lower_bound) | (df['Cost'] > upper_bound)]
print("Number of outliers after imputation:", len(outliers_after_imputation))

# Check for duplicates
duplicates = df.duplicated()
print("Number of duplicates:", duplicates.sum())

# View the first few duplicate rows
df_cleaned = df.drop_duplicates()
df = df_cleaned
print(df.duplicated().sum())

"""Subsequently, an in-depth Exploratory Data Analysis (EDA) is recommended to uncover patterns and relationships within the data.

Analysing Numerical data
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for Quantity
plt.figure(figsize=(6, 2))
sns.histplot(df['Quantity'], bins=2, kde=True)
plt.title('Distribution of Quantity')
plt.xlabel('Quantity')
plt.ylabel('Frequency')
plt.show()

# Box plot for UnitPrice
plt.figure(figsize=(12, 6))
sns.boxplot(x=df['UnitPrice'])
plt.title('Box Plot of UnitPrice')
plt.xlabel('UnitPrice')
plt.show()

# Box plot for Cost
plt.figure(figsize=(12, 6))
sns.boxplot(x=df['Cost'])
plt.title('Box Plot of Cost')
plt.xlabel('Cost')
plt.show()

"""Analysing Categorical data"""

# Value counts for InvoiceNo
print(df['InvoiceNo'].value_counts().head(10))  # Check the top 10 Invoice numbers

# Scatter plot between Quantity and Cost
plt.figure(figsize=(12, 6))
sns.scatterplot(x='Quantity', y='Cost', data=df)
plt.title('Scatter Plot of Quantity vs Cost')
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.show()

"""Relation B/w the variables"""

# Convert 'InvoiceDate' to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Extract year and month
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month

# Group by year and month and calculate total sales
sales_over_time = df.groupby(['Year', 'Month'])['Cost'].sum().reset_index()

# Create a 'Year-Month' column for plotting purposes
sales_over_time['YearMonth'] = sales_over_time['Year'].astype(str) + '-' + sales_over_time['Month'].astype(str).str.zfill(2)
print(len(sales_over_time['YearMonth'].unique()))

# Plotting sales over time
plt.figure(figsize=(14, 7))
plt.plot(sales_over_time['YearMonth'], sales_over_time['Cost'])
plt.title('Total Sales Over Time (Year and Month)')
plt.xlabel('Year-Month')
plt.ylabel('Total Sales (Cost)')
plt.xticks(rotation=45)
plt.grid()
plt.show()

"""We have the maximum sales in 2011-11"""

# Group by 'Country' and calculate total sales
country_sales = df.groupby('Country')['Cost'].sum().reset_index()

# Plotting total sales for each 'Country'
plt.figure(figsize=(14, 7))
plt.bar(country_sales['Country'], country_sales['Cost'])
plt.title('Total Sales by Country')
plt.xlabel('Country')
plt.ylabel('Total Sales (Cost)')
plt.xticks(rotation=90)
plt.grid(axis='y')
plt.show()

"""We can see the highest no of slaes in UK.

Within the context of Customer Relationship Management (CRM) analytics, prioritize customer-centric feature engineering. This includes creating metrics like Recency, Frequency, and Monetary values, offering insights into customer transaction patterns.
"""

import pandas as pd

# Assuming 'InvoiceDate' is already in datetime format
current_date = pd.Timestamp.now()  # Keep current_date as a pandas Timestamp

# Calculate Recency for each customer
df['Recency'] = (current_date - df.groupby("CustomerID")['InvoiceDate'].transform('max')).dt.days
df['Frequency'] = df.groupby('CustomerID')['InvoiceNo'].transform('nunique')
df['Monetary'] = df.groupby('CustomerID')['Cost'].transform('sum')
print("Recency: min =", df['Recency'].min(), ", max =", df['Recency'].max())
print("Frequency: min =", df['Frequency'].min(), ", max =", df['Frequency'].max())
print("Monetary: min =", df['Monetary'].min(), ", max =", df['Monetary'].max())

"""**Additional Features**"""

df['AvgPurchaseValue'] = df['Monetary'] / df['Frequency']
df['Tenure'] = (current_date - df.groupby('CustomerID')['InvoiceDate'].transform('min')).dt.days
churn_threshold = 180  # e.g., 180 days
df['ChurnIndicator'] = (df['Recency'] > churn_threshold).astype(int)
df.head()

"""Customer Value Analysis:
The calculation of 'AvgPurchaseValue' (Monetary / Frequency) is a key metric in understanding customer behavior. This insight allows businesses to identify:

High-value customers who make large purchases but may buy infrequently
Frequent buyers who may make smaller purchases but contribute significantly over time
Potential for upselling or cross-selling to different customer segments
"""

# Binning Recency with duplicate edges dropped
df['Recency_Binned'] = pd.cut(df['Recency'], bins=10, labels=False, duplicates='drop')

# Binning Frequency with duplicate edges dropped
df['Frequency_Binned'] = pd.cut(df['Frequency'], bins=10, labels=False, duplicates='drop')

# Binning Monetary with duplicate edges dropped
df['Monetary_Binned'] = pd.cut(df['Monetary'], bins=10, labels=False, duplicates='drop')
df.head()

"""Data Discretization:
The code is converting continuous variables (Recency, Frequency, Monetary) into categorical variables by binning them into 10 categories each. This process, known as discretization, can be useful for:

Simplifying complex continuous data
Reducing the impact of small fluctuations in the data
Preparing data for certain machine learning algorithms that work better with categorical data

Extend the analysis to incorporate unique product purchase details, derive RFM (Recency, Frequency, Monetary) scores for customer segmentation, and categorize them based on activity levels.
"""

# Create a combined RFM segment score
df['RFM_Segment'] = df['Recency_Binned'].astype(str) + df['Frequency_Binned'].astype(str) + df['Monetary_Binned'].astype(str)

# Display the first few records to check the segment scores
print(df[['CustomerID', 'Recency_Binned', 'Frequency_Binned', 'Monetary_Binned', 'RFM_Segment']].head())
def assign_segment(row):
    if row['Recency_Binned'] == 0 and row['Frequency_Binned'] == 0 and row['Monetary_Binned'] == 0:
        return 'Best Customers'
    elif row['Recency_Binned'] <= 2 and row['Frequency_Binned'] >= 2 and row['Monetary_Binned'] >= 2:
        return 'Loyal Customers'
    elif row['Recency_Binned'] <= 2 and row['Frequency_Binned'] == 1:
        return 'New Customers'
    elif row['Recency_Binned'] >= 3:
        return 'At Risk Customers'
    else:
        return 'Others'

# Apply the function to create a segment column
df['Customer_Segment'] = df.apply(assign_segment, axis=1)

# Display the first few records to check the segments
print(df[['CustomerID', 'Recency_Binned', 'Frequency_Binned', 'Monetary_Binned', 'Customer_Segment']].head())

"""The segmentation process simplifies complex customer interactions into manageable, meaningful categories, allowing businesses to tailor their approaches based on customer value and engagement levels. This framework provides a foundation for more personalized marketing, targeted retention efforts, and efficient resource allocation.
The flexibility built into the segmentation logic allows for customization and evolution of the model as business needs change. While it offers immediate actionable insights, it also sets the stage for more advanced analytics and predictive modeling in the future.

Integrate additional customer-centric features such as average days between purchases, preferred shopping days, and peak shopping hours for a holistic understanding.

**1.Average days between purchases**
"""

# Convert InvoiceDate to datetime if not already done
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Sort values by CustomerID and InvoiceDate
df.sort_values(by=['CustomerID', 'InvoiceDate'], inplace=True)

# Calculate the difference between consecutive purchases
df['Days_Between_Purchases'] = df.groupby('CustomerID')['InvoiceDate'].diff().dt.days

# Calculate the average days between purchases for each customer
avg_days_between = df.groupby('CustomerID')['Days_Between_Purchases'].mean().reset_index()
avg_days_between.rename(columns={'Days_Between_Purchases': 'Avg_Days_Between_Purchases'}, inplace=True)

# Merge this feature back to the main DataFrame
df = df.merge(avg_days_between, on='CustomerID', how='left')
df.head()

"""The analysis introduces a temporal dimension to the customer behavior study, moving beyond static RFM metrics to understand the rhythm of customer interactions with the business. By calculating the time between purchases and the average interval for each customer, it reveals patterns in shopping frequency and consistency.
This temporal analysis can uncover valuable insights about customer engagement, loyalty, and potential churn risks. It allows businesses to identify customers with regular purchasing habits versus those with sporadic interactions, enabling more nuanced segmentation and personalized marketing strategies.

**2.preferred shopping days**
"""

# Extract the day of the week from InvoiceDate
df['Shopping_Day'] = df['InvoiceDate'].dt.day_name()

# Determine the preferred shopping day for each customer
preferred_shopping_day = df.groupby('CustomerID')['Shopping_Day'].agg(lambda x: x.mode()[0]).reset_index()
preferred_shopping_day.rename(columns={'Shopping_Day': 'Preferred_Shopping_Day'}, inplace=True)

# Merge this feature back to the main DataFrame
df = df.merge(preferred_shopping_day, on='CustomerID', how='left')
df.head()

"""Understanding preferred shopping days can provide valuable insights for various aspects of business operations and marketing strategies. It allows for a more nuanced approach to customer engagement, moving beyond just the frequency and value of purchases to consider the timing of customer interactions.
This information can be leveraged to optimize various business processes:

Marketing and Promotions: Tailoring marketing communications and promotions to align with customers' preferred shopping days can increase engagement and conversion rates.
Inventory Management: Adjusting stock levels based on daily shopping patterns can improve inventory efficiency and reduce costs.
Staffing and Resource Allocation: Businesses can optimize staffing levels and resource allocation to match peak shopping days for different customer segments.
Personalization: Incorporating preferred shopping days into customer profiles enhances the ability to deliver personalized experiences and recommendations.
Customer Segmentation: This data adds another layer to customer segmentation, allowing for more refined and targeted marketing strategies.

3.peak shopping hours
"""

# Extract hour from InvoiceDate
df['Shopping_Hour'] = df['InvoiceDate'].dt.hour

# Determine the peak shopping hour for each customer
peak_shopping_hour = df.groupby('CustomerID')['Shopping_Hour'].agg(lambda x: x.mode()[0]).reset_index()
peak_shopping_hour.rename(columns={'Shopping_Hour': 'Peak_Shopping_Hour'}, inplace=True)

# Merge this feature back to the main DataFrame
df = df.merge(peak_shopping_hour, on='CustomerID', how='left')

customer_analysis = avg_days_between.merge(preferred_shopping_day, on='CustomerID', how='left')\
                                     .merge(peak_shopping_hour, on='CustomerID', how='left')
print(customer_analysis)

"""This analysis of peak shopping hours can provide valuable behavioral insights into customer habits, helping businesses optimize their marketing, operations, and customer engagement strategies. By leveraging this data, businesses can target customers more effectively and enhance their overall shopping experience."""